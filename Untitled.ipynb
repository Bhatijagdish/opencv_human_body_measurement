{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b2ae9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2824916380.py, line 236)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 236\u001b[1;36m\u001b[0m\n\u001b[1;33m    val_loss = val_loss_h + val_loss_m + val_loss_j +\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loss import dice_coef, dice_loss\n",
    "from dataset import Images\n",
    "from network import UNet\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\"\"\"\n",
    "Training Height Estimation Network\n",
    "\n",
    "Example Run:\n",
    "CUDA_VISIBLE_DEVICES=1 python train.py -e 50 -l 1e-3 -ls mse -d [DIRECTORY OF TRAINING CSV FILE] -r 0\n",
    "\"\"\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # PARSER SETTINGS\n",
    "    parser = argparse.ArgumentParser(description='U-Net PyTorch Model for Height and Weight Prediction in IMDB Dataset')\n",
    "\n",
    "    parser.add_argument('-e', '--epoch', type=int, required=True, help='Number of Epochs')\n",
    "    parser.add_argument('-l', '--learning_rate', type=float, required=True, help='Learning rate')\n",
    "    parser.add_argument('-ls', '--loss', type=str, required=True, help='Height loss type')\n",
    "    parser.add_argument('-d', '--dataset', type=str, required=True, help='Dataset directory')\n",
    "\n",
    "    parser.add_argument('-w1', '--w1_loss', type=float, default=1, help='Loss weight for Height Estimation')\n",
    "    parser.add_argument('-w2', '--w2_loss', type=float, default=1, help='Loss weight for Cross Entropy')\n",
    "    parser.add_argument('-w3', '--w3_loss', type=float, default=1, help='Loss weight for Dice Loss')\n",
    "    \n",
    "    parser.add_argument('-m', '--min_neuron', type=int, default=128, help='Minimum neuron number for the first layer')\n",
    "    parser.add_argument('-bs', '--batch_size', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('-r', '--resume', type=int, required=True, help='Continue training')\n",
    "    parser.add_argument('-pr', '--pretrained', type=str, help='Load pretrained model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # INITIALIZATIONS\n",
    "    n_epochs = args.epoch\n",
    "    \n",
    "    w1_loss = args.w1_loss\n",
    "    w2_loss = args.w2_loss\n",
    "    w3_loss = args.w3_loss\n",
    "        \n",
    "    \n",
    "    if args.loss == 'mse':\n",
    "        height_loss = nn.MSELoss()\n",
    "    elif args.loss == 'mae':\n",
    "        height_loss = nn.L1Loss()\n",
    "    elif args.loss == 'huber':\n",
    "        height_loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    train = DataLoader(Images(args.dataset, 'TRAINING.csv', True), \n",
    "                       batch_size=args.batch_size, num_workers=8, shuffle=True)\n",
    "    \n",
    "    valid = DataLoader(Images(args.dataset, 'VAL.csv', True), \n",
    "                       batch_size=1, num_workers=8, shuffle=False)\n",
    "    \n",
    "    \n",
    "    print(\"Training on \" + str(len(train)*args.batch_size) + \" images.\")\n",
    "    print(\"Validating on \" + str(len(valid)) + \" images.\")\n",
    "\n",
    "    net = UNet(args.min_neuron)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    #pretrained_model = torch.load(glob('models/IMDB_MODEL_06102019_121502/*')[0])\n",
    "    #state_dict = pretrained_model[\"state_dict\"]\n",
    "    \n",
    "    #own_state = net.state_dict()\n",
    "    \n",
    "    #for name, param in state_dict.items():\n",
    "    #    if name not in own_state:\n",
    "    #         continue\n",
    "    #    if isinstance(param, Parameter):\n",
    "            # backwards compatibility for serialized parameters\n",
    "    #        param = param.data\n",
    "            \n",
    "    #    if not ((\"height_1\" in name) or (\"height_2\" in name)):\n",
    "    #        own_state[name].copy_(param)\n",
    "        \n",
    "    #for param in net.parameters():\n",
    "    #    param.requires_grad = False\n",
    "       \n",
    "    #net.conv_out.height_1[0].weight.requires_grad = True\n",
    "    #net.conv_out.height_1[0].bias.requires_grad = True\n",
    "    #net.conv_out.height_2[0].weight.requires_grad = True\n",
    "    #net.conv_out.height_2[0].bias.requires_grad = True\n",
    "    #net.conv_out.height_2[3].weight.requires_grad = True\n",
    "    #net.conv_out.height_2[3].bias.requires_grad = True\n",
    "    \n",
    "    SAVE_DIR = 'IMDB_MODEL_' + datetime.now().strftime(\"%d%m%Y_%H%M%S\") + '/'\n",
    "    \n",
    "    MODEL_SETTINGS = {\n",
    "        'epoch': n_epochs,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'mask_loss_weight': w1_loss,\n",
    "        'joint_loss_weight': w2_loss,\n",
    "        'height_loss_weight': w3_loss,\n",
    "        'height_loss': args.loss,\n",
    "        'batch_size': args.batch_size,\n",
    "        'dataset': args.dataset,\n",
    "        'min_neuron': args.min_neuron\n",
    "    }\n",
    "    \n",
    "    LOG_DIR = 'logs/' + SAVE_DIR\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(LOG_DIR)\n",
    "        np.save(LOG_DIR + 'model_settings.npy', MODEL_SETTINGS)\n",
    "    except:\n",
    "        print(\"Error ! Model exists.\")\n",
    "    \n",
    "    # Print Number of Parameters\n",
    "    n_params = 0\n",
    "\n",
    "    for param in net.parameters():\n",
    "        n_params += param.numel()\n",
    "        \n",
    "    print('Total params:', n_params)\n",
    "    print('Trainable params:', sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "    print('Non-trainable params:',n_params-sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "    \n",
    "    # Use GPU\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    " \n",
    "    # MODEL TRAINING\n",
    "    \n",
    "    if args.resume == 1:\n",
    "        pretrained_model = torch.load(glob(args.pretrained)[0])\n",
    "        net.load_state_dict(pretrained_model[\"state_dict\"])\n",
    "        start_epoch = pretrained_model[\"epoch\"]\n",
    "        v_l = pretrained_model[\"v_l\"]\n",
    "        v_lh = pretrained_model[\"v_h\"]\n",
    "        v_lm = pretrained_model[\"v_m\"]\n",
    "        v_lj = pretrained_model[\"v_j\"]\n",
    "\n",
    "        t_l = pretrained_model[\"t_l\"]\n",
    "        t_lh = pretrained_model[\"t_h\"]\n",
    "        t_lm = pretrained_model[\"t_m\"]\n",
    "        t_lj = pretrained_model[\"t_j\"]\n",
    "    else:\n",
    "        v_l = []\n",
    "        v_lh = []\n",
    "        v_lm = []\n",
    "        v_lj = []\n",
    "\n",
    "        t_l = []\n",
    "        t_lh = []\n",
    "        t_lm = []\n",
    "        t_lj = []\n",
    "        \n",
    "        t_acc = []\n",
    "        v_acc = []\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_ep = -1\n",
    "    \n",
    "    for ep in range(start_epoch, start_epoch+n_epochs):\n",
    "\n",
    "        with tqdm(total=len(train), dynamic_ncols=True) as progress:\n",
    "            \n",
    "            loss_ = 0.\n",
    "            tm_ = 0.\n",
    "            tj_ = 0.\n",
    "            th_ = 0.\n",
    "            acc_ = 0.\n",
    "            \n",
    "            progress.set_description('Epoch: %s' % str(ep+1))\n",
    "\n",
    "            for idx, batch_data in enumerate(train):\n",
    "                X, y_mask, y_joint, y_height = batch_data['img'].cuda(), batch_data['mask'].cuda(), batch_data['joint'].cuda(), batch_data['height'].cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                mask_o, joint_o, height_o = net(X)\n",
    "                \n",
    "                loss_m = w1_loss * (dice_loss(mask_o, y_mask, 0) + dice_loss(mask_o, y_mask, 1))/2\n",
    "                loss_j = w2_loss * nn.CrossEntropyLoss()(joint_o, y_joint)  \n",
    "                loss_h = w3_loss * height_loss(height_o, y_height)\n",
    "\n",
    "                loss = loss_h + loss_m + loss_j  \n",
    "                \n",
    "                pred = torch.argmax(height_o, 1)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                progress.update(1)\n",
    "                \n",
    "                loss_ += loss.item()\n",
    "                tm_ += loss_m.item()\n",
    "                tj_ += loss_j.item()\n",
    "                th_ += loss_h.item()\n",
    "                \n",
    "                progress.set_postfix(loss=loss_/(idx+1), mask=tm_/(idx+1), joint=tj_/(idx+1), height=th_/(idx+1), acc=acc_/(idx+1))\n",
    "\n",
    "            loss_ /= len(train)\n",
    "            tm_ /= len(train)\n",
    "            tj_ /= len(train)\n",
    "            th_ /= len(train)\n",
    "            acc_ /= len(train)\n",
    "            \n",
    "        progress.write('Validating ...')\n",
    "        \n",
    "        net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            vl_ = 0.\n",
    "            vm_ = 0.\n",
    "            vj_ = 0.\n",
    "            vh_ = 0.\n",
    "            val_acc_ = 0.\n",
    "\n",
    "            for idx, batch_data in enumerate(valid):\n",
    "                X, y_mask, y_joint, y_height = batch_data['img'].cuda(), batch_data['mask'].cuda(), batch_data['joint'].cuda(), batch_data['height'].cuda()\n",
    "              \n",
    "                mask_o, joint_o, height_o = net(X)\n",
    "\n",
    "                val_loss_m = w1_loss * (dice_loss(mask_o, y_mask, 0) + dice_loss(mask_o, y_mask, 1))/2\n",
    "                val_loss_j = w2_loss * nn.CrossEntropyLoss()(joint_o, y_joint)\n",
    "                val_loss_h = w3_loss * height_loss(height_o, y_height)\n",
    "                \n",
    "                val_loss = val_loss_h + val_loss_m + val_loss_j + \n",
    "                \n",
    "                pred = torch.argmax(height_o, 1)\n",
    "\n",
    "                vl_ += val_loss.item()\n",
    "                vm_ += val_loss_m.item()\n",
    "                vj_ += val_loss_j.item()\n",
    "                vh_ += val_loss_h.item()\n",
    "\n",
    "            vl_ /= len(valid)\n",
    "            vm_ /= len(valid)\n",
    "            vj_ /= len(valid)\n",
    "            vh_ /= len(valid)\n",
    "            \n",
    "        t_l.append(loss_)\n",
    "        t_lm.append(tm_)\n",
    "        t_lj.append(tj_)\n",
    "        t_lh.append(th_)\n",
    "\n",
    "        v_l.append(vl_)\n",
    "        v_lm.append(vm_)\n",
    "        v_lj.append(vj_)\n",
    "        v_lh.append(vh_)\n",
    "\n",
    "        if vl_ < best_val:\n",
    "\n",
    "            best_val = vl_\n",
    "\n",
    "            state = {'epoch': ep + 1, \n",
    "                     'state_dict': net.state_dict(),\n",
    "                     'optimizer': optimizer.state_dict(),\n",
    "                     't_l': t_l,\n",
    "                     't_m': t_lm,\n",
    "                     't_j': t_lj,\n",
    "                     't_h': t_lh,\n",
    "                     'v_l': v_l,\n",
    "                     'v_m': v_lm,\n",
    "                     'v_j': v_lj,\n",
    "                     'v_h': v_lh\n",
    "                    }\n",
    "\n",
    "            if os.path.exists('models/' + SAVE_DIR):\n",
    "                os.remove('models/' + SAVE_DIR + 'model_ep_{}.pth.tar'.format(best_ep))\n",
    "            else:\n",
    "                os.makedirs('models/' + SAVE_DIR)\n",
    "\n",
    "            torch.save(state, 'models/' + SAVE_DIR + 'model_ep_{}.pth.tar'.format(ep+1))\n",
    "            best_ep = ep+1\n",
    "                \n",
    "\n",
    "        progress.write('T Loss: {:.3f} - T Mask: {:.3f} - T Joint: {:.3f} - T Height: {:.3f}\\nV Loss: {:.3f} - V Mask: {:.3f} - V Joint: {:.3f} - V Height: {:.3f}'.format(loss_, tm_, tj_, th_, vl_, vm_, vj_, vh_))\n",
    "            \n",
    "        net.train()\n",
    "   \n",
    "    np.save(LOG_DIR + 't_loss', np.array(t_l))\n",
    "    np.save(LOG_DIR + 't_mask', np.array(t_lm))\n",
    "    np.save(LOG_DIR + 't_joint', np.array(t_lj))\n",
    "    np.save(LOG_DIR + 't_height', np.array(t_lh))\n",
    "    \n",
    "    np.save(LOG_DIR + 'v_loss', np.array(v_l))\n",
    "    np.save(LOG_DIR + 'v_mask', np.array(v_lm))\n",
    "    np.save(LOG_DIR + 'v_joint', np.array(v_lj))\n",
    "    np.save(LOG_DIR + 'v_height', np.array(v_lh))\n",
    "    \n",
    "    state = {'epoch': start_epoch+n_epochs, \n",
    "             'state_dict': net.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(),\n",
    "             't_l': t_l,\n",
    "             't_m': t_lm,\n",
    "             't_j': t_lj,\n",
    "             't_h': t_lh,\n",
    "             'v_l': v_l,\n",
    "             'v_m': v_lm,\n",
    "             'v_j': v_lj,\n",
    "             'v_h': v_lh\n",
    "    }\n",
    "\n",
    "    torch.save(state, 'models/' + SAVE_DIR + 'last_model_ep_{}.pth.tar'.format(start_epoch+n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loss import dice_coef, dice_loss\n",
    "from dataset import Images\n",
    "from network import UNet\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\"\"\"\n",
    "Training Height Estimation Network\n",
    "\n",
    "Example Run:\n",
    "CUDA_VISIBLE_DEVICES=1 python train.py -e 50 -l 1e-3 -ls mse -d [DIRECTORY OF TRAINING CSV FILE] -r 0\n",
    "\"\"\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # PARSER SETTINGS\n",
    "    parser = argparse.ArgumentParser(description='U-Net PyTorch Model for Height and Weight Prediction in IMDB Dataset')\n",
    "\n",
    "    parser.add_argument('-e', '--epoch', type=int, required=True, help='Number of Epochs')\n",
    "    parser.add_argument('-l', '--learning_rate', type=float, required=True, help='Learning rate')\n",
    "    parser.add_argument('-ls', '--loss', type=str, required=True, help='Height loss type')\n",
    "    parser.add_argument('-d', '--dataset', type=str, required=True, help='Dataset directory')\n",
    "    parser.add_argument('-w1', '--w1_loss', type=float, default=1, help='Loss weight for Height Estimation')\n",
    "    parser.add_argument('-w2', '--w2_loss', type=float, default=1, help='Loss weight for Cross Entropy')\n",
    "    parser.add_argument('-w3', '--w3_loss', type=float, default=1, help='Loss weight for Dice Loss')\n",
    "    parser.add_argument('-m', '--min_neuron', type=int, default=128, help='Minimum neuron number for the first layer')\n",
    "    parser.add_argument('-bs', '--batch_size', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('-r', '--resume', type=int, required=True, help='Continue training')\n",
    "    parser.add_argument('-pr', '--pretrained', type=str, help='Load pretrained model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # INITIALIZATIONS\n",
    "    n_epochs = args.epoch\n",
    "    \n",
    "    w1_loss = args.w1_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
